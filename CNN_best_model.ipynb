{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The goal of our project is to build a Convolutional Neural Network (CNN) model capable of achieving over 95% accuracy to classify voxelized point cloud pairs as either \"similar\" (1) or \"dissimilar\" (0). The data consists of pairs of voxelized point clouds along with corresponding labels.\n",
    "\n",
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voxelization\n",
    "The following function takes a point cloud and voxelizes it into a 32x32x32 resolution voxel grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxelize(pc):\n",
    "    # Normalize point cloud data\n",
    "    point_cloud = (pc - pc.min(axis=0)) / (pc.max(axis=0) - pc.min(axis=0))\n",
    "    voxel_size = 32\n",
    "    voxelized = np.zeros((voxel_size, voxel_size, voxel_size))\n",
    "    point_cloud = ((voxel_size - 1) * point_cloud).astype(int)\n",
    "    voxelized[point_cloud[:, 0], point_cloud[:, 1], point_cloud[:, 2]] = 1\n",
    "    return voxelized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "The following class does the following tasks:\n",
    "- Loads pairs of voxelized point clouds from `.npy` files.\n",
    "- Uses the voxelization function to convert the point clouds.\n",
    "- Converts the voxelized data to PyTorch tensors.\n",
    "\n",
    "The dataset is loaded from the `curated_data` directory as provided in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelDataset(Dataset):\n",
    "    def __init__(self, file_list, data_directory, subset_size=None): \n",
    "        self.data_directory = data_directory\n",
    "        self.file_list = file_list\n",
    "        if subset_size is not None:\n",
    "            self.file_list = self.file_list[:subset_size]  # Use only a subset of files \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_directory, self.file_list[idx])\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        part1, part2, label = data\n",
    "\n",
    "        # Voxelizing the point clouds\n",
    "        voxel1 = voxelize(part1)\n",
    "        voxel2 = voxelize(part2)\n",
    "\n",
    "        # Convert to tensors\n",
    "        voxel1 = torch.tensor(voxel1, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        voxel2 = torch.tensor(voxel2, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return voxel1, voxel2, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split, DataLoader and Making a Subset\n",
    "We split the dataset into **70% training**, **15% validation**, and **15% test** to evaluate model performance effectively.\n",
    "We also use a dataloader and pass a subset of data through it. This is useful when parameter tuning because passing all of the data through the model takes a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_directory = os.path.join(os.getcwd(), \"curated_data/\")\n",
    "\n",
    "# Get all files in the directory\n",
    "all_files = [f for f in os.listdir(data_directory) if f.endswith('.npy')]\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_files, test_files = train_test_split(all_files, test_size=0.3, random_state=42)\n",
    "val_files, test_files = train_test_split(test_files, test_size=0.5, random_state=42)  # Further split to validation and test\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = VoxelDataset(train_files, data_directory) \n",
    "val_dataset = VoxelDataset(val_files, data_directory)     \n",
    "test_dataset = VoxelDataset(test_files, data_directory)   \n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training on a subset\n",
    "total_num_files = len(train_dataset)\n",
    "subset_size = int(total_num_files * 0.1) # Tunable: for subset size. put 1 for 100% of data\n",
    "def get_subset_sampler(dataset_size, subset_size):\n",
    "    indices = np.arange(dataset_size)\n",
    "    np.random.shuffle(indices)\n",
    "    subset_indices = indices[:subset_size]\n",
    "    return SubsetRandomSampler(subset_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Encoder Class\n",
    "We define the 3D Encoder model that:\n",
    "- Uses 3D convolutional layers to extract features from the voxelized input.\n",
    "- Contains two convolutional layers with ReLU activation.\n",
    "- The feature maps are flattened, followed by a fully connected (fc) layer that produces a feature vector.\n",
    "- Prints the shape after each layer for debugging purposes.\n",
    "\n",
    "### Finetuning\n",
    "Some of the comments in this class show code we wrote for finetuning. It has been left as comments to showcase some approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 8, kernel_size=3, stride=1, padding=1) # Tunable: Number of filters (4). More filters may capture more features but are computationally expensive. Make changes in progressions like 2, 4, 8, 16, 32,...\n",
    "        #self.pool = nn.MaxPool3d(kernel_size=2, stride=2)  # Pooling layer to reduce spatial dimensions by half\n",
    "        self.conv2 = nn.Conv3d(8, 16, kernel_size=3, stride=2, padding=1)  # Tunable: Number of filters (8). Increasing may improve feature extraction at the cost of increased computation.\n",
    "        #self.dropout = nn.Dropout(p=0.1)\n",
    "        #self.conv3 = nn.Conv3d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc = nn.Linear(16 * 16 * 16 * 16, 32)  # Tunable: Output features (32). Increasing this can capture more complex relationships but increases model complexity.\n",
    "\n",
    "        self.first_forward = True  # Flag to track the first forward pass. We don't want the model to print these values after every epoch.\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        if self.first_forward:\n",
    "            print(f\"Shape after conv1: {x.shape}\")\n",
    "        #x = self.pool(x)  # Apply max pooling to reduce spatial dimensions\n",
    "        #if self.first_forward:\n",
    "            #print(f\"Shape after pooling: {x.shape}\")\n",
    "        x = F.relu(self.conv2(x))\n",
    "        if self.first_forward:\n",
    "            print(f\"Shape after conv2: {x.shape}\")\n",
    "        #x = self.pool(x)  # Apply max pooling again to reduce spatial dimensions\n",
    "        #if self.first_forward:\n",
    "            #print(f\"Shape after pooling: {x.shape}\")\n",
    "        x = x.view(x.size(0), -1)  # Flatten for the fully connected layer\n",
    "        #if self.first_forward:\n",
    "            #print(f\"Shape after dropout: {x.shape}\")\n",
    "        #x = self.dropout(x)  # Drop some of the features to prevent overfitting\n",
    "        if self.first_forward:\n",
    "            print(f\"Shape after flattening: {x.shape}\")\n",
    "        x = self.fc(x)\n",
    "        if self.first_forward:\n",
    "            print(f\"Shape after fc: {x.shape}\")\n",
    "            self.first_forward = False  # Disable further shape printing after the first forward pass\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Network\n",
    "The Siamese Network is built using two instances of the shared encoder. It compares the output of both encoders using L1 distance and computes a similarity score. The subnets both have the same weights and architectures. A sigmoid activation function is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese3DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Siamese3DCNN, self).__init__()\n",
    "        self.encoder = Encoder3D()\n",
    "        self.fc_out = nn.Linear(32, 1)  # Tunable: Number of input features (32). Match this to the output of the encoder.\n",
    "        \n",
    "\n",
    "    def forward(self, voxel1, voxel2):\n",
    "        encoded1 = self.encoder(voxel1)\n",
    "        encoded2 = self.encoder(voxel2)\n",
    "\n",
    "        # Compute the L1 distance between the two encoded vectors\n",
    "        combined = torch.abs(encoded1 - encoded2)\n",
    "\n",
    "        # Pass through fully connected layer to get a single value\n",
    "        output = self.fc_out(combined)\n",
    "\n",
    "        # Apply sigmoid to produce a probability output\n",
    "        output = torch.sigmoid(output)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Class\n",
    "\n",
    "This class encapsulates the entire training and evaluation process for the Siamese 3D CNN. It includes methods for training, validating, saving the best model, and plotting metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese3DTrainer:\n",
    "    def __init__(self, model, train_dataset, val_dataset, test_dataset, learning_rate=0.001, batch_size=128):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device('cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=1e-5)\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        self.best_model_path = \"best_siamese_3dcnn_model.pth\"\n",
    "        #self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        training_losses, validation_losses = [], []\n",
    "        training_accuracies, validation_accuracies = [], []\n",
    "        best_train_accuracy = 0.0\n",
    "        best_validation_accuracy = 0.0\n",
    "\n",
    "        for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
    "            #for loading subset of data\n",
    "            train_sampler = get_subset_sampler(len(train_dataset), subset_size)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "            # Training Phase\n",
    "            self.model.train()\n",
    "            train_loss, correct_preds, total_preds = 0.0, 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            for voxel1, voxel2, labels in tqdm(self.train_loader, leave=False, desc=f\"Epoch {epoch+1} Train Batches\"):\n",
    "                # Move data to appropriate device\n",
    "                voxel1, voxel2, labels = voxel1.to(self.device), voxel2.to(self.device), labels.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(voxel1, voxel2)\n",
    "                labels = labels.view(-1, 1)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                # Calculate training accuracy\n",
    "                predictions = (outputs > 0.5).float()\n",
    "                correct_preds += (predictions == labels).sum().item()\n",
    "                total_preds += labels.size(0)\n",
    "\n",
    "            avg_train_loss = train_loss / len(self.train_loader)\n",
    "            train_accuracy = correct_preds / total_preds\n",
    "            training_losses.append(avg_train_loss)\n",
    "            training_accuracies.append(train_accuracy)\n",
    "\n",
    "            print(f\"\\nEpoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.6f}, Training Accuracy: {train_accuracy:.6f}, Time Taken: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "            # Validation Phase\n",
    "            self.model.eval()\n",
    "            val_loss, correct_preds, total_preds = 0.0, 0, 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for voxel1, voxel2, labels in tqdm(self.val_loader, leave=False, desc=f\"Epoch {epoch+1} Validation Batches\"):\n",
    "                    # Move data to appropriate device\n",
    "                    voxel1, voxel2, labels = voxel1.to(self.device), voxel2.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.model(voxel1, voxel2)\n",
    "                    labels = labels.view(-1, 1)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    # Calculate validation accuracy\n",
    "                    predictions = (outputs > 0.5).float()\n",
    "                    correct_preds += (predictions == labels).sum().item()\n",
    "                    total_preds += labels.size(0)\n",
    "\n",
    "            avg_val_loss = val_loss / len(self.val_loader)\n",
    "            val_accuracy = correct_preds / total_preds\n",
    "            validation_losses.append(avg_val_loss)\n",
    "            validation_accuracies.append(val_accuracy)\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {avg_val_loss:.6f}, Validation Accuracy: {val_accuracy:.6f}\")\n",
    "\n",
    "            # Early Stopping and Save Best Model (validation)\n",
    "            if max(validation_accuracies) > best_validation_accuracy:\n",
    "                best_validation_accuracy = max(validation_accuracies)\n",
    "                torch.save(self.model.state_dict(), self.best_model_path)\n",
    "                print(f\"Best model saved at epoch {epoch + 1} with Validation Accuracy: {best_validation_accuracy:.6f}\")\n",
    "            else:\n",
    "                print(f\"Validation accuracy decreased. Stopping early at epoch {epoch + 1}.\")\n",
    "                break\n",
    "\n",
    "        # Plot training/validation loss and accuracy\n",
    "        self.plot_metrics(training_losses, validation_losses, training_accuracies, validation_accuracies)\n",
    "\n",
    "    def evaluate(self):\n",
    "        # Load the best saved model\n",
    "        self.model.load_state_dict(torch.load(self.best_model_path))\n",
    "        self.model.eval()\n",
    "\n",
    "        all_labels, all_predictions = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for voxel1, voxel2, labels in self.test_loader:\n",
    "                voxel1, voxel2, labels = voxel1.to(self.device), voxel2.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(voxel1, voxel2)\n",
    "                predictions = (outputs > 0.5).float()\n",
    "\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    def plot_metrics(self, training_losses, validation_losses, training_accuracies, validation_accuracies):\n",
    "        epochs = range(1, len(training_losses) + 1)\n",
    "\n",
    "        plt.figure(figsize=(14, 7))\n",
    "\n",
    "        plt.figure(figsize=(14, 10))\n",
    "\n",
    "        # Plot Training & Validation Loss\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(epochs, training_losses, 'b', label='Training Loss')\n",
    "        plt.plot(epochs, validation_losses, 'r', label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot Training & Validation Accuracy\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(epochs, training_accuracies, 'b', label='Training Accuracy')\n",
    "        plt.plot(epochs, validation_accuracies, 'r', label='Validation Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Model Architecture to understand parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Encoder3D: 1-1                         --\n",
      "|    └─Conv3d: 2-1                       224\n",
      "|    └─Conv3d: 2-2                       3,472\n",
      "|    └─Linear: 2-3                       2,097,184\n",
      "├─Linear: 1-2                            33\n",
      "=================================================================\n",
      "Total params: 2,100,913\n",
      "Trainable params: 2,100,913\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Encoder3D: 1-1                         --\n",
       "|    └─Conv3d: 2-1                       224\n",
       "|    └─Conv3d: 2-2                       3,472\n",
       "|    └─Linear: 2-3                       2,097,184\n",
       "├─Linear: 1-2                            33\n",
       "=================================================================\n",
       "Total params: 2,100,913\n",
       "Trainable params: 2,100,913\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Siamese3DCNN()\n",
    "summary(model, input_size=[(1, 32, 32, 32), (1, 32, 32, 32)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Model and Trainer, Train the Model and Validate\n",
    "\n",
    "Here we are training and validating the model on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after conv1: torch.Size([512, 8, 32, 32, 32])\n",
      "Shape after conv2: torch.Size([512, 16, 16, 16, 16])\n",
      "Shape after flattening: torch.Size([512, 65536])\n",
      "Shape after fc: torch.Size([512, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/15], Training Loss: 0.367198, Training Accuracy: 0.789318, Time Taken: 167.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   7%|▋         | 1/15 [03:00<42:00, 180.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Validation Loss: 0.216300, Validation Accuracy: 0.907171\n",
      "Best model saved at epoch 1 with Validation Accuracy: 0.907171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/15], Training Loss: 0.150920, Training Accuracy: 0.932924, Time Taken: 152.43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  13%|█▎        | 2/15 [05:45<37:08, 171.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Validation Loss: 0.164234, Validation Accuracy: 0.931842\n",
      "Best model saved at epoch 2 with Validation Accuracy: 0.931842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/15], Training Loss: 0.081762, Training Accuracy: 0.957254, Time Taken: 169.90s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  20%|██        | 3/15 [08:50<35:32, 177.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/15], Validation Loss: 0.205121, Validation Accuracy: 0.941041\n",
      "Best model saved at epoch 3 with Validation Accuracy: 0.941041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/15], Training Loss: 0.056201, Training Accuracy: 0.966574, Time Taken: 161.86s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  27%|██▋       | 4/15 [11:48<32:34, 177.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/15], Validation Loss: 0.179997, Validation Accuracy: 0.947941\n",
      "Best model saved at epoch 4 with Validation Accuracy: 0.947941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Tunable parameters:\n",
    "    # `learning_rate`: A smaller learning rate (0.001) helps the model converge slowly and stably. Higher rates (e.g., 0.01) may lead to faster but unstable convergence.\n",
    "    # `batch_size`: Larger batch sizes help with convergence and are faster but need more memory.\n",
    "\n",
    "# Create the model and trainer\n",
    "model = Siamese3DCNN()\n",
    "trainer = Siamese3DTrainer(model=model,\n",
    "                           train_dataset=train_dataset,\n",
    "                           val_dataset=val_dataset,\n",
    "                           test_dataset=test_dataset,\n",
    "                           learning_rate=0.001,\n",
    "                           batch_size=512)\n",
    "\n",
    "# Train the model for 15 epochs\n",
    "trainer.train(num_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2t/yzzf0l8s4y5cv5y7k66sf0980000gn/T/ipykernel_10167/4163804464.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(self.best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.47%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best saved model\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
